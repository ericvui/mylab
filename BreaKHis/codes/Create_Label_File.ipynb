{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features\n",
    "import numpy as np\n",
    "def extract_label(filename):\n",
    "    parts = filename.split(\"_\")\n",
    "    if parts[1] == \"B\":\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    sub_parts = parts[2].split(\"-\")\n",
    "    size = np.int(sub_parts[3])\n",
    "    return label, size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare files in csv file\n",
    "import os\n",
    "src_directory = \"/tf/dataset/classes\"\n",
    "dest_directory = \"/tf/dataset/classes\"\n",
    "sub_directory = [\"40_patch_4\",\"100_patch_4\",\"200_patch_4\",\"400_patch_4\"]\n",
    "for sub_dir in sub_directory:\n",
    "    curr_sub_dir = os.path.join(src_directory,sub_dir)\n",
    "    curr_filename = \"data_file_\" + sub_dir + \".csv\"\n",
    "    #curr_filename = \"data_file_Fake_\" + sub_dir + \".csv\"\n",
    "    content = []\n",
    "    # extract image label\n",
    "    for path,_,ls_files in os.walk(curr_sub_dir):\n",
    "        for file in ls_files:\n",
    "            label, size = extract_label(file)\n",
    "            line = (\"{},{},{}\".format(os.path.join(curr_sub_dir,file),label,size)) \n",
    "            content.append(line)\n",
    "    # write to file        \n",
    "    with open(os.path.join(dest_directory,curr_filename), 'w') as writer:\n",
    "        for line in content:\n",
    "            writer.write(line + \"\\n\")    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test generated label data\n",
    "import sys\n",
    "sys.path.append('../') \n",
    "from codes.data.create_dataset_01 import create_dataset\n",
    "directory = \"/tf/dataset/classes\"\n",
    "dataset = create_dataset(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(40, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(400, shape=(), dtype=int32)\n",
      "tf.Tensor(200, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n",
      "tf.Tensor(100, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "directory = \"F:\\\\GraduateClass\\\\Thesis\\\\Dataset\\\\BreaKHis\\\\histology_slides\\\\classes\"\n",
    "data_file_list = [\"data_file_40.csv\",\"data_file_100.csv\",\"data_file_200.csv\",\"data_file_400.csv\"]\n",
    "rate = 0.7\n",
    "epoches = 5\n",
    "for epoch in range(epoches):\n",
    "    fn_train_ep = \"train_data_file_ep_{}.csv\" . format(epoch)\n",
    "    fn_test_ep = \"test_data_file_ep_{}.csv\" . format(epoch)\n",
    "    print(\"Epoch {}\" . format(epoch))\n",
    "    for data_file in data_file_list:\n",
    "        with open(os.path.join(directory,data_file), \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        sub_train, sub_test = train_test_split(lines,test_size=(1-rate))\n",
    "\n",
    "        with open(os.path.join(directory,fn_train_ep), 'a') as writer1:\n",
    "            for line in sub_train:\n",
    "                writer1.write(line)        \n",
    "\n",
    "        with open(os.path.join(directory,fn_test_ep), 'a') as writer2:\n",
    "            for line in sub_test:\n",
    "                writer2.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide images into equal-size images\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "src_path = \"/tf/dataset/classes/400\"\n",
    "dest_path = \"/tf/dataset/classes/400_patch_9\"\n",
    "photo_size = (460,700)\n",
    "division = 2\n",
    "size_rows = np.int(photo_size[0]/division)\n",
    "size_cols = np.int(photo_size[1]/division)\n",
    "\n",
    "for path,_,files_list in os.walk(src_path):\n",
    "    \n",
    "    for filename in files_list:\n",
    "        \n",
    "        filename_no_ext = filename.split(\".\")[0]\n",
    "        \n",
    "        in_im = Image.open(os.path.join(src_path,filename))\n",
    "        \n",
    "        imarray = np.array(in_im)\n",
    "        stt = 0\n",
    "        for i in range(division):\n",
    "            for j in range(division):\n",
    "                sub_imarray = imarray[i*size_rows:i*size_rows+size_rows,j*size_cols:j*size_cols+size_cols,:]\n",
    "                ou_im = Image.fromarray(sub_imarray)\n",
    "                ou_im.save(os.path.join(dest_path,\"{}-{}.png\" \n",
    "                                        . format(filename_no_ext,stt)))\n",
    "                stt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = \"/tf/dataset/classes\"\n",
    "epoch = 2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "full_img_data = ['40','100','200','400']\n",
    "\n",
    "for i_ep in range(epoch):\n",
    "    train_filename = \"train_data_file_ep_{}.csv\" . format(i_ep)\n",
    "    test_filename = \"test_data_file_ep_{}.csv\" . format(i_ep)\n",
    "    f_train = open(os.path.join(directory,'epoch',train_filename),'a')\n",
    "    f_test = open(os.path.join(directory,'epoch',test_filename),'a')\n",
    "    for img_size in full_img_data:\n",
    "\n",
    "        #read full images\n",
    "        list_img = []\n",
    "        with open(os.path.join(directory,'data_file_{}.csv' . format(img_size)), \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        for ln in lines:\n",
    "            line = ln.strip().split('/')[-1]\n",
    "            list_img.append(line)\n",
    "\n",
    "        #convert images into 4 patch images\n",
    "        d_train, d_test = train_test_split(list_img,test_size=0.3, random_state=42)\n",
    "        for x in d_train:\n",
    "            label, size = extract_label(x)\n",
    "            fn_without_ext = x.split(\".\")[0]\n",
    "            for i_patch in range(4):\n",
    "                fn_with_ext = \"{}/{}-{}.png,{},{}\" .format(os.path.join(directory,img_size+'_patch') ,fn_without_ext,i_patch,label,size)\n",
    "                f_train.write(fn_with_ext + \"\\n\")\n",
    "\n",
    "        for y in d_test:\n",
    "            fn_without_ext = y.split(\".\")[0]\n",
    "            label, size = extract_label(y)\n",
    "            for i_patch in range(4):\n",
    "                fn_with_ext = \"{}/{}-{}.png,{},{}\" .format(os.path.join(directory,img_size+'_patch') ,fn_without_ext,i_patch,label,size)\n",
    "                f_test.write(fn_with_ext + \"\\n\")\n",
    "            \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide images into 2 images per horizontally, vertically\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "src_path = \"/tf/dataset/classes/400\"\n",
    "dest_path = \"/tf/dataset/classes/400_patch_4\"\n",
    "photo_size = (460,700)\n",
    "division = 2\n",
    "size_rows = np.int(photo_size[0]/division)\n",
    "size_cols = np.int(photo_size[1]/division)\n",
    "\n",
    "for path,_,files_list in os.walk(src_path):\n",
    "    \n",
    "    for filename in files_list:\n",
    "        \n",
    "        filename_no_ext = filename.split(\".\")[0]\n",
    "        \n",
    "        in_im = Image.open(os.path.join(src_path,filename))\n",
    "        \n",
    "        imarray = np.array(in_im)\n",
    "        stt = 0\n",
    "        #divide image by horizontally and then vertically axis\n",
    "        for i in range(division):\n",
    "            sub_imarray_horizontal = imarray[i*size_rows:i*size_rows+size_rows,:,:]\n",
    "            sub_imarray_vertical = imarray[:,i*size_cols:i*size_cols+size_cols,:]\n",
    "\n",
    "            ou_im_horizontal = Image.fromarray(sub_imarray_horizontal)\n",
    "            ou_im_horizontal.save(os.path.join(dest_path,\"{}-{}.png\" \n",
    "                                    . format(filename_no_ext,stt)))\n",
    "            ou_im_vertical = Image.fromarray(sub_imarray_vertical)\n",
    "            ou_im_vertical.save(os.path.join(dest_path,\"{}-{}.png\" \n",
    "                                    . format(filename_no_ext,stt+2)))\n",
    "            stt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create fake file csv from all fake images generated by stylegan\n",
    "\n",
    "#extract features\n",
    "import numpy as np\n",
    "def extract_fake_label(filename):\n",
    "    parts = filename.split(\"_\")\n",
    "    if parts[1] == \"B\":\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    sub_parts = parts[2].split(\"-\")\n",
    "    size = np.int(sub_parts[3])\n",
    "    return label, size\n",
    "\n",
    "import os\n",
    "dest_path = \"/tf/dataset/classes/\"\n",
    "content = []\n",
    "output_filename = 'data_file_fake.csv'\n",
    "\n",
    "#Write B cancer\n",
    "src_path = \"/tf/Downloads/b_output_stylegan/\"\n",
    "folders = ['B100','B40','B200','B400']\n",
    "for dir_path in folders:\n",
    "    for path,_,files_list in os.walk(os.path.join(src_path,dir_path)):\n",
    "        for filename in files_list:\n",
    "            \n",
    "            label, size = extract_fake_label(filename)\n",
    "            full_filename = os.path.join(path,filename)\n",
    "            line = (\"{},{},{}\".format(full_filename,label,size)) \n",
    "            content.append(line)\n",
    "#Write M cancer\n",
    "src_path = \"/tf/Downloads/m_output_stylegan/\"\n",
    "folders = ['M100','M40','M200','M400']\n",
    "for dir_path in folders:\n",
    "    for path,_,files_list in os.walk(os.path.join(src_path,dir_path)):\n",
    "        for filename in files_list:\n",
    "            \n",
    "            label, size = extract_fake_label(filename)\n",
    "            \n",
    "            full_filename = os.path.join(path,filename)\n",
    "            line = (\"{},{},{}\".format(full_filename,label,size)) \n",
    "            content.append(line)            \n",
    "                        \n",
    "# write all to file        \n",
    "with open(os.path.join(dest_path,output_filename), 'w') as writer:\n",
    "    for line in content:\n",
    "        writer.write(line + \"\\n\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 400\n"
     ]
    }
   ],
   "source": [
    "filename = 'fake_B_F-F-F-400-433.png'\n",
    "\n",
    "parts = filename.split(\"_\")\n",
    "if parts[1] == \"B\":\n",
    "    label = 1\n",
    "else:\n",
    "    label = 0\n",
    "sub_parts = parts[2].split(\"-\")\n",
    "size = np.int(sub_parts[3])\n",
    "print(label,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide BACH images\n",
    "\n",
    "# divide images into 2 images per horizontally, vertically\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "src_path = \"/tf/dataset/BACH/Invasive\"\n",
    "dest_path = \"/tf/dataset/BACH/Invasive_png\"\n",
    "photo_size = (2048,1536)\n",
    "division = 2\n",
    "size_rows = np.int(photo_size[0]/division)\n",
    "size_cols = np.int(photo_size[1]/division)\n",
    "\n",
    "for path,_,files_list in os.walk(src_path):\n",
    "    \n",
    "    for filename in files_list:\n",
    "        if filename.endswith('.tif'):\n",
    "            filename_no_ext = filename.split(\".\")[0]\n",
    "            #print(filename_no_ext)\n",
    "            in_im = Image.open(os.path.join(src_path,filename))\n",
    "\n",
    "            imarray = np.array(in_im)\n",
    "            stt = 0\n",
    "            #divide image by horizontally and then vertically axis\n",
    "            for i in range(division):\n",
    "                sub_imarray_horizontal = imarray[i*size_rows:i*size_rows+size_rows,:,:]\n",
    "                sub_imarray_vertical = imarray[:,i*size_cols:i*size_cols+size_cols,:]\n",
    "\n",
    "                ou_im_horizontal = Image.fromarray(sub_imarray_horizontal)\n",
    "                ou_im_horizontal.save(os.path.join(dest_path,\"{}-{}.png\" \n",
    "                                        . format(filename_no_ext,stt)))\n",
    "                ou_im_vertical = Image.fromarray(sub_imarray_vertical)\n",
    "                ou_im_vertical.save(os.path.join(dest_path,\"{}-{}.png\" \n",
    "                                        . format(filename_no_ext,stt+2)))\n",
    "                stt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create BACH file csv for training\n",
    "\n",
    "#extract features\n",
    "import numpy as np\n",
    "def extract_BACH_label(filename):\n",
    "    parts = filename\n",
    "    if parts[0] == \"b\" or parts[0] == \"n\":\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    return label\n",
    "\n",
    "import os\n",
    "src_directory = \"/tf/dataset/BACH\"\n",
    "dest_directory = \"/tf/dataset/BACH\"\n",
    "sub_directory = [\"Normal\",\"Invasive\",\"InSitu\",\"Benign\"]\n",
    "for sub_dir in sub_directory:\n",
    "    curr_sub_dir = os.path.join(src_directory,sub_dir)\n",
    "    curr_filename = \"data_file_\" + sub_dir + \".csv\"\n",
    "    #curr_filename = \"data_file_Fake_\" + sub_dir + \".csv\"\n",
    "    content = []\n",
    "    # extract image label\n",
    "    for path,_,ls_files in os.walk(curr_sub_dir):\n",
    "        for file in ls_files:\n",
    "            if file.endswith('.tif'):\n",
    "                label = extract_BACH_label(file)\n",
    "                line = (\"{},{}\".format(os.path.join(curr_sub_dir,file),label)) \n",
    "                content.append(line)\n",
    "    # write to file        \n",
    "    with open(os.path.join(dest_directory,curr_filename), 'w') as writer:\n",
    "        for line in content:\n",
    "            writer.write(line + \"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create fake file csv from all fake images generated by pix2pix\n",
    "\n",
    "#extract features\n",
    "import numpy as np\n",
    "def extract_fake_label(filename):\n",
    "    parts = filename.split(\"_\")\n",
    "    if parts[1] == \"B\":\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    sub_parts = parts[2].split(\"-\")\n",
    "    size = np.int(sub_parts[3])\n",
    "    return label, size\n",
    "\n",
    "import os\n",
    "dest_path = \"/tf/dataset/classes/\"\n",
    "content = []\n",
    "output_filename = 'data_file_fake_pix2pix.csv'\n",
    "\n",
    "#Write B cancer\n",
    "src_path = \"/tf/dataset/gen_breakhis\"\n",
    "folders = ['fake_40','fake_100','fake_200','fake_400']\n",
    "for dir_path in folders:\n",
    "    for path,_,files_list in os.walk(os.path.join(src_path,dir_path)):\n",
    "        for filename in files_list:\n",
    "            \n",
    "            label, size = extract_fake_label(filename)\n",
    "            full_filename = os.path.join(path,filename)\n",
    "            line = (\"{},{},{}\".format(full_filename,label,size)) \n",
    "            content.append(line)\n",
    "\n",
    "# write all to file        \n",
    "with open(os.path.join(dest_path,output_filename), 'w') as writer:\n",
    "    for line in content:\n",
    "        writer.write(line + \"\\n\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
